{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cc9dbcc",
   "metadata": {},
   "source": [
    "# 13th_June_BigData_Assignment.ipynb\n",
    "**Data Science Masters – Big Data Assignment**\n",
    "\n",
    "This notebook contains answers to the Big Data assignment questions including theoretical explanations and practical implementations using Python and Apache Spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b916efa7",
   "metadata": {},
   "source": [
    "## Q1: Core Components of the Hadoop Ecosystem\n",
    "\n",
    "Hadoop consists of the following core components:\n",
    "\n",
    "1. **HDFS (Hadoop Distributed File System)** – Stores data across multiple machines ensuring fault tolerance.\n",
    "2. **MapReduce** – A programming model for processing large data sets with a distributed algorithm.\n",
    "3. **YARN (Yet Another Resource Negotiator)** – Manages cluster resources and job scheduling.\n",
    "\n",
    "- **HDFS** provides high-throughput access to application data.\n",
    "- **MapReduce** processes the data in two phases: Map and Reduce.\n",
    "- **YARN** separates resource management from data processing components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed95d9e",
   "metadata": {},
   "source": [
    "## Q2: Hadoop Distributed File System (HDFS)\n",
    "\n",
    "### Key Components:\n",
    "- **NameNode**: Manages metadata and directory structure of HDFS.\n",
    "- **DataNode**: Stores actual data blocks.\n",
    "- **Blocks**: Data is split into blocks (default 128MB), replicated across DataNodes.\n",
    "\n",
    "### Reliability and Fault Tolerance:\n",
    "- Data replication ensures that even if a DataNode fails, data can be retrieved from replicas.\n",
    "- The NameNode monitors block health and schedules replication if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ca3a6a",
   "metadata": {},
   "source": [
    "## Q3: MapReduce Framework\n",
    "\n",
    "### Step-by-Step:\n",
    "1. **Map Phase**: Input data is split and processed into key-value pairs.\n",
    "2. **Shuffle and Sort**: System groups values by key.\n",
    "3. **Reduce Phase**: Aggregates values for each key to produce the final result.\n",
    "\n",
    "### Example: Word Count\n",
    "- Map: (word, 1)\n",
    "- Reduce: Sum counts for each word\n",
    "\n",
    "### Advantages:\n",
    "- Scalable, fault-tolerant, batch processing\n",
    "\n",
    "### Limitations:\n",
    "- Slow for iterative tasks, high latency, complex for simple operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c41530",
   "metadata": {},
   "source": [
    "## Q4: Role of YARN\n",
    "\n",
    "YARN manages resources and schedules tasks in a Hadoop cluster.\n",
    "\n",
    "### Components:\n",
    "- **ResourceManager**: Allocates resources.\n",
    "- **NodeManager**: Manages node-level execution.\n",
    "- **ApplicationMaster**: Coordinates individual jobs.\n",
    "\n",
    "### Comparison with Hadoop 1.x:\n",
    "- Hadoop 1.x had a monolithic JobTracker.\n",
    "- YARN enables multiple applications (not just MapReduce).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d841729e",
   "metadata": {},
   "source": [
    "## Q5: Hadoop Ecosystem Components\n",
    "\n",
    "- **HBase**: NoSQL database on HDFS.\n",
    "- **Hive**: SQL-like interface for querying HDFS data.\n",
    "- **Pig**: High-level scripting for data analysis.\n",
    "- **Spark**: Fast, in-memory big data processing.\n",
    "\n",
    "### Example: Using Hive with Hadoop\n",
    "Hive enables SQL-like queries over large datasets stored in HDFS, improving accessibility for users familiar with SQL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64eb30d",
   "metadata": {},
   "source": [
    "## Q6: Apache Spark vs. Hadoop MapReduce\n",
    "\n",
    "| Feature          | Spark                            | MapReduce                        |\n",
    "|------------------|----------------------------------|----------------------------------|\n",
    "| Speed            | In-memory, faster                | Disk-based, slower               |\n",
    "| Ease of Use      | APIs in Python, Scala, Java      | Java-based, verbose              |\n",
    "| Iterative Jobs   | Efficient                        | Inefficient                      |\n",
    "| Machine Learning | MLlib                            | Not suitable                     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7d5d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "text_file = sc.textFile(\"sample.txt\")\n",
    "\n",
    "word_counts = (text_file\n",
    "               .flatMap(lambda line: line.split())\n",
    "               .map(lambda word: (word.lower(), 1))\n",
    "               .reduceByKey(lambda a, b: a + b)\n",
    "               .sortBy(lambda x: -x[1]))\n",
    "\n",
    "word_counts.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99218563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming we have a dataset as RDD\n",
    "data = sc.parallelize([\n",
    "    (\"John\", 25), (\"Jane\", 30), (\"Joe\", 45), (\"Jill\", 28)\n",
    "])\n",
    "\n",
    "# a. Filter\n",
    "filtered = data.filter(lambda x: x[1] > 30)\n",
    "\n",
    "# b. Map\n",
    "mapped = data.map(lambda x: (x[0], x[1] + 5))\n",
    "\n",
    "# c. Reduce (Average Age)\n",
    "total = data.map(lambda x: x[1]).reduce(lambda a, b: a + b)\n",
    "count = data.count()\n",
    "average = total / count\n",
    "filtered.collect(), mapped.collect(), average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb87c9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"people.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# a. Select\n",
    "df.select(\"Name\", \"Age\").show()\n",
    "\n",
    "# b. Filter\n",
    "df.filter(df.Age > 30).show()\n",
    "\n",
    "# c. Group and Aggregate\n",
    "df.groupBy(\"Department\").avg(\"Salary\").show()\n",
    "\n",
    "# d. Join\n",
    "df2 = spark.read.csv(\"departments.csv\", header=True, inferSchema=True)\n",
    "df.join(df2, on=\"DepartmentID\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cffebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "counts = (lines.flatMap(lambda line: line.split())\n",
    "               .map(lambda word: (word, 1))\n",
    "               .reduceByKey(lambda a, b: a + b))\n",
    "\n",
    "counts.pprint()\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc89b1b",
   "metadata": {},
   "source": [
    "## Q11: Apache Kafka Concepts\n",
    "\n",
    "Kafka is a distributed streaming platform that enables:\n",
    "- Publish-subscribe messaging\n",
    "- Fault-tolerant storage\n",
    "- Real-time processing\n",
    "\n",
    "Kafka addresses the need for real-time, durable, high-throughput data pipelines in big data systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4366f8d",
   "metadata": {},
   "source": [
    "## Q12: Kafka Architecture\n",
    "\n",
    "- **Producers**: Send messages to topics.\n",
    "- **Topics**: Logical channel for messages.\n",
    "- **Brokers**: Kafka servers storing messages.\n",
    "- **Consumers**: Read messages from topics.\n",
    "- **ZooKeeper**: Coordinates cluster state and leader election.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4848a505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Producer Example (Python)\n",
    "from kafka import KafkaProducer\n",
    "producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
    "producer.send('test-topic', b'Hello, Kafka!')\n",
    "producer.flush()\n",
    "\n",
    "# Consumer Example\n",
    "from kafka import KafkaConsumer\n",
    "consumer = KafkaConsumer('test-topic', bootstrap_servers='localhost:9092')\n",
    "for message in consumer:\n",
    "    print(message.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4a6803",
   "metadata": {},
   "source": [
    "## Q14: Kafka Data Retention and Partitioning\n",
    "\n",
    "- **Data Retention**: Configurable duration messages stay in a topic (default 7 days).\n",
    "- **Partitioning**: Splits data for parallelism. Each partition is an ordered log.\n",
    "\n",
    "Retention and partitioning improve scalability and manageability in Kafka.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fe27bc",
   "metadata": {},
   "source": [
    "## Q15: Real-World Use Cases\n",
    "\n",
    "- **LinkedIn**: Activity tracking\n",
    "- **Uber**: Real-time analytics\n",
    "- **Netflix**: Event sourcing\n",
    "- **Financial Services**: Fraud detection\n",
    "\n",
    "Kafka offers scalability, durability, and real-time capabilities crucial for modern data systems.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
